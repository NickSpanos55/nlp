{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f12049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0b467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "usc_dir = \"/home/grilio/Documents/SLP/Lab_2/kaldi/egs/usc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89aa5667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grilio/Documents/SLP/Lab_2/kaldi/egs/usc'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b534a10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grilio/Documents/SLP/Lab_2/kaldi/egs/usc'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(usc_dic)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df4530",
   "metadata": {},
   "source": [
    "# 3 Preparatory Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a560a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')\n",
    "\n",
    "folders = ['./data/dev','./data/train','./data/test']\n",
    "for folder in folders:\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except OSError as error: \n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e30874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./lexicon.txt','r') as f:\n",
    "    lexicon = {}\n",
    "    lines = f.readlines()\n",
    "    print(lines)\n",
    "    for line in lines:\n",
    "        key = line.split('\\t')[0]\n",
    "        value = line.split('\\t')[1].replace('\\n', '').strip()\n",
    "        lexicon[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da1f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['train','dev','test'] \n",
    "for direc in dirs:\n",
    "    \n",
    "    if(direc == 'train'):\n",
    "        path = 'training'\n",
    "    elif(direc == 'dev'):\n",
    "        path = 'validation'\n",
    "    else:\n",
    "        path = 'testing'\n",
    "        \n",
    "    uttids = []\n",
    "    with open('./filesets/{}.txt'.format(path),'r') as f:\n",
    "        speakers = f.readlines()\n",
    "        speakers = [speaker.strip('\\n') for speaker in speakers]\n",
    "        for speaker in speakers:\n",
    "            uttids.append(speaker)\n",
    "\n",
    "    with open('./transcriptions.txt','r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            line = re.sub(r\"[^a-zA-Z ']+\", '', lines[i]).lower()\n",
    "            lines[i] = line\n",
    "\n",
    "    with open('./data/{}/utt2spk'.format(direc),'w') as f:\n",
    "        for uttid in uttids:\n",
    "            f.write(\"{} {}\\n\".format(uttid,uttid[0:2]))\n",
    "\n",
    "    with open('./data/{}/wav.scp'.format(direc),'w') as f:\n",
    "        for uttid in uttids:\n",
    "            f.write(\"{} {}\\n\".format(uttid,'./wav/{}.wav'.format(uttid)))\n",
    "\n",
    "    with open('./data/{}/text'.format(direc),'w') as f:\n",
    "        for uttid in uttids:\n",
    "            line = lines[int(uttid[-3:])-1].split(' ')\n",
    "            start = 'sil'\n",
    "            for word in line:\n",
    "                start = start + ' ' + lexicon[word.upper()]\n",
    "            start = start + ' sil'\n",
    "            f.write(\"{} {}\\n\".format(uttid,start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3c6e4",
   "metadata": {},
   "source": [
    "# 4 Main lab steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20cc26",
   "metadata": {},
   "source": [
    "## 4.1 Preparing the USC-TIMIT speech recognition system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f3e39",
   "metadata": {},
   "source": [
    "5. Create the folder local and create a soft link to the file `steps/score_kaldi.sh` inside `local`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69f4715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(usc_dir + \"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2fe945",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './local'\n",
    "os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96242333",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(usc_dir + \"/data/local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8dcfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the user\n",
    "! ln -s /home/grilio/Documents/SLP/Lab_2/kaldi/egs/usc/steps/score_kaldi.sh score_kaldi_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7338b0b",
   "metadata": {},
   "source": [
    "6. Create the folder `conf` and copy inside it the file `mfcc.conf` provided to you in the helper code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "950e0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './conf'\n",
    "os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "048be320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-06 20:58:51--  https://github.com/slp-ntua/slp-labs/blob/master/lab2/mfcc.conf\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘mfcc.conf’\n",
      "\n",
      "mfcc.conf               [ <=>                ] 143,79K  --.-KB/s    in 0,08s   \n",
      "\n",
      "2023-05-06 20:58:52 (1,71 MB/s) - ‘mfcc.conf’ saved [147244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/slp-ntua/slp-labs/blob/master/lab2/mfcc.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9e2d1",
   "metadata": {},
   "source": [
    "7. Finally, create the following folders: `data/lang`, `data/local/dict`, `data/local/lm_tmp`, `data/local/nist_lm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96ac408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(usc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94c3ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['./data/lang','./data/local/dict', './data/local/lm_tmp', './data/local/nist_lm']\n",
    "for folder in folders:\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except OSError as error: \n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813ac42",
   "metadata": {},
   "source": [
    "## 4.2 Building the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef441dc",
   "metadata": {},
   "source": [
    "1. Inside the folder `data/local/dict` you will put the files needed for constructing the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188dd54e",
   "metadata": {},
   "source": [
    "- Create `silence_phones.txt` and `optional_silence.txt` containing only the silence phoneme (`sil`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32db253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/local/dict/silence_phones.txt','w') as f:\n",
    "    f.write(\"sil\\n\")\n",
    "with open('./data/local/dict/optional_silence.txt','w') as f:\n",
    "    f.write(\"sil\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9c856",
   "metadata": {},
   "source": [
    "- Create `nonsilence_phones.txt`, which contains all other phonemes (one per line, sorted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8a988",
   "metadata": {},
   "source": [
    "- Create `lexicon.txt` containing the vocabulary. Since we perform phoneme recognition (and not word recognition), `lexicon.txt` contains a 1-1 mapping of each phoneme to itself. In each line of `lexicon.txt` you must put a phoneme then a `<space>` and then the same phoneme again. Do not forget to include the silence phoneme (`sil`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fb17161",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes = []\n",
    "with open('./lexicon.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        key = line.split('\\t')[0]\n",
    "        value = line.split('\\t')[1].replace('\\n', '').strip().split(' ')\n",
    "        for phoneme in value:\n",
    "            if(phoneme not in phonemes):\n",
    "                phonemes.append(phoneme)\n",
    "                \n",
    "phonemes.sort()       \n",
    "with open('./data/local/dict/nonsilence_phones.txt','w') as f:\n",
    "    for phoneme in phonemes:\n",
    "        if(phoneme != \"sil\"):\n",
    "            f.write(\"{}\\n\".format(phoneme))\n",
    "\n",
    "with open('./data/local/dict/lexicon.txt','w') as f:\n",
    "    for phoneme in phonemes:\n",
    "        if(phoneme != '<oov>'):\n",
    "            f.write(\"{} {}\\n\".format(phoneme,phoneme))\n",
    "        else:\n",
    "            f.write(\"{} {}\\n\".format(phoneme,'sil'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd19856",
   "metadata": {},
   "source": [
    "- To create the `lm_train.text`, you need to add special tokens `<s>` and `</s>` in the beginning and the end of each line respectively, based on the file text that you created during the lab preparation. Similarly, you also need to create `lm_dev.text` and `lm_test.text` files in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff28042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['train','dev','test'] \n",
    "for direc in dirs:\n",
    "    \n",
    "    if(direc == 'train'):\n",
    "        path = 'training'\n",
    "    elif(direc == 'dev'):\n",
    "        path = 'validation'\n",
    "    else:\n",
    "        path = 'testing'\n",
    "        \n",
    "    uttids = []\n",
    "    with open('./filesets/{}.txt'.format(path),'r') as f:\n",
    "        speakers = f.readlines()\n",
    "        speakers = [speaker.strip('\\n') for speaker in speakers]\n",
    "        for speaker in speakers:\n",
    "            uttids.append(speaker)\n",
    "\n",
    "    with open('./transcriptions.txt','r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            line = re.sub(r\"[^a-zA-Z ']+\", '', lines[i]).lower()\n",
    "            lines[i] = line\n",
    "            \n",
    "    with open('./data/local/lm_tmp/lm_{}.txt'.format(direc),'w') as f:\n",
    "        for uttid in uttids:\n",
    "            line = lines[int(uttid[-3:])-1].split(' ')\n",
    "            start = 'sil'\n",
    "            for word in line:\n",
    "                start = start + ' ' + lexicon[word.upper()]\n",
    "            start = start + ' sil'\n",
    "            f.write(\"{} <s> {} </s>\\n\".format(uttid,start))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639be8c",
   "metadata": {},
   "source": [
    "- Finally, create an empty file `extra_questions.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4b9b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/extra_questions.txt','x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
